# -*- coding: utf-8 -*-
"""DNN_Regression .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IRwLtK6fpUcTY1ov3c_vkWUvhKogfjE5

#**Regression using TensorFlow**

---
In a regression problem, the aim is to predict the output of a continuous value, like a price or a probability

regression is used to predict the value of a variable based on the value of another variable. The variable you want to predict is called the dependent variable. The variable you are using to predict the other variable's value is called the independent variable.

---
"""

!pip install -q seaborn

import matplotlib.pyplot as plt   # Importing matplotlib which will help us in data visualization
import numpy as np                # numpy is a Numerical programming package for the Python and will help us with large array vectors
import pandas as pd               # pandas is a Python library for data manipulation and analysis
import seaborn as sns             # Seaborn is a Python data visualization library based on matplotlib

# Make NumPy printouts easier to read.
np.set_printoptions(precision=3, suppress=True)

import tensorflow as tf         #Machine learning software library

from tensorflow import keras   # Keras Neural network library
from tensorflow.keras import layers

print(tf.__version__)         # It is important to note the TF Verson for compatibility issues

"""##Importing the Datset

"""

#importing the dataset directly from cloud 
#url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'
column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',
                'Acceleration', 'Model Year', 'Origin']

csv = '/content/auto-mpg.csv' # importing the file locally
raw_dataset = pd.read_csv(csv,na_values='?')

dataset = raw_dataset.copy()
dataset.tail()
# all of our vehicle attributes can be seen here that we will be using to find the milage

"""##Filtering the data"""

dataset.isna().sum()

dataset = dataset.dropna()

"""Lets drop the origin as this is categorical not numeric. after this the orgin will be placed into 3 different columns Europe, Japan and USA"""

dataset['Origin'] = dataset['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})

dataset = pd.get_dummies(dataset, columns=['Origin'], prefix='', prefix_sep='')
dataset.tail()

"""##Splitting the data into Test and Train Split


---



we will split the dataset where one part is used to build the model and the rest to test it. 
"""

train_dataset = dataset.sample(frac=0.8, random_state=0)
test_dataset = dataset.drop(train_dataset.index)

"""##Visualizing the data

---
"""

sns.pairplot(train_dataset[['MPG', 'Cylinders', 'Displacement', 'Weight']], diag_kind='kde')

train_dataset.describe().transpose()

train_features = train_dataset.copy()
test_features = test_dataset.copy()

train_labels = train_features.pop('MPG')
test_labels = test_features.pop('MPG')

"""##Normalization

---


 reason this is important is because the features are multiplied by the model weights. So, the scale of the outputs and the scale of the gradients are affected by the scale of the inputs.
"""

train_dataset.describe().transpose()[['mean', 'std']] # Normalization

normalizer = tf.keras.layers.Normalization(axis=-1)

normalizer.adapt(np.array(train_features))

print(normalizer.mean.numpy())

first = np.array(train_features[:1])

with np.printoptions(precision=2, suppress=True):
  print('First example:', first)
  print()
  print('Normalized:', normalizer(first).numpy())

"""##Regression with multiple inputs """

model_dnn = tf.keras.Sequential([
    normalizer,
    layers.Dense(units=1)
])

model_dnn.predict(train_features[:10])

model_dnn.layers[1].kernel

"""

---


configuring the model with keras ***Model.compile*** and ***Model.fit*** for 100 epochs 


---

"""

model_dnn.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),
    loss='mean_absolute_error')

def plot_loss(history):
  plt.plot(history.history['loss'], label='loss')
  plt.plot(history.history['val_loss'], label='val_loss')
  plt.ylim([0, 10])
  plt.xlabel('Epoch')
  plt.ylabel('Error [MPG]')
  plt.legend()
  plt.grid(True)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# history = model_dnn.fit(
#     train_features,
#     train_labels,
#     epochs=100,
#     # Suppress logging.
#     verbose=0,
#     # Calculate validation results on 20% of the training data.
#     validation_split = 0.2)

plot_loss(history)

"""##Regression with a deep neural network (DNN)"""

def build_and_compile_model(norm):
  model = keras.Sequential([
      norm,
      layers.Dense(64, activation='relu'), # here we use Relu(rectified linear unit) activation finction
      layers.Dense(64, activation='relu'),
      layers.Dense(1)
  ])

  model.compile(loss='mean_absolute_error',
                optimizer=tf.keras.optimizers.Adam(0.001)) #Optimizer that implements the Adam algorithm.
  return model

dnn_model = build_and_compile_model(normalizer)
dnn_model.summary()

"""## Training the model with Keras ***Model.fit***


"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# history = dnn_model.fit(
#     train_features,
#     train_labels,
#     validation_split=0.2,
#     verbose=0, epochs=100)

plot_loss(history)

test_results = {}
test_results['dnn_model'] = dnn_model.evaluate(test_features, test_labels, verbose=0)
pd.DataFrame(test_results, index=['Mean absolute error [MPG]']).T

"""##**Make Predictions**

---



we can now make predictions with the dnn_model on the test set using ***Model.predict***
"""

test_predictions = dnn_model.predict(test_features).flatten()

a = plt.axes(aspect='equal')
plt.scatter(test_labels, test_predictions)
plt.xlabel('True Values [MPG]')
plt.ylabel('Predictions [MPG]')
lims = [0, 50]
plt.xlim(lims)
plt.ylim(lims)
_ = plt.plot(lims, lims)

"""##Checking the Error Distribution

---


"""

error = test_predictions - test_labels
plt.hist(error, bins=25)
plt.xlabel('Prediction Error [MPG]')
_ = plt.ylabel('Count')

"""##Saving the model

---


"""

dnn_model.save('dnn_model') # saving the TF model for later use

dot_img_file = '/content/model_1.png' #saving the NN representation image locally
tf.keras.utils.plot_model(dnn_model, to_file=dot_img_file, show_shapes=True)